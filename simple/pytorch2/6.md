FullyShardedDataParallel（简称FSDP）是PyTorch中一种高级的数据并行训练技术，主要用于大规模分布式训练中对模型参数、优化器状态和梯度进行“完全分片”（fully sharded），以显著降低单个GPU的内存占用，从而支持训练更大规模的模型或使用更大的批量大小。

## 什么是FullyShardedDataParallel（FSDP）

- FSDP是一种数据并行训练方法，但与传统的DistributedDataParallel（DDP）不同，DDP会在每个GPU上完整复制模型参数、梯度和优化器状态，而FSDP将这些状态在不同GPU（或进程）间进行切分（shard），每个GPU只保存自己负责的那部分数据[1][2][4][7]。
- 它借鉴了ZeRO Stage 3的思想，能够将模型参数、梯度和优化器状态都分布在多个GPU上，极大地减少了每个设备的内存压力[1][5]。
- FSDP支持将不活跃的参数、梯度和优化器状态卸载到CPU内存，进一步节省GPU内存[3][4]。
- 在前向和反向传播过程中，FSDP会动态地通过通信（all_gather、reduce_scatter等操作）收集和分发参数和梯度，完成计算后立即释放不需要的内存，最大化内存利用效率[2][3][4]。

## FSDP的工作流程简述

- **构造阶段**：将模型参数切分成若干份，分配给不同的GPU，每个GPU只保留自己负责的参数切片。
- **前向传播**：每个FSDP实例通过all_gather操作收集所有GPU上的参数切片，恢复完整参数，执行前向计算，计算完成后释放非本地参数切片。
- **反向传播**：同样先all_gather恢复完整参数，执行反向计算计算梯度，然后通过reduce_scatter操作将梯度切分并分发到各GPU，释放非本地参数。
- **优化器更新**：每个GPU仅更新自己负责的参数切片[2][3][4]。

## 通常什么情况下使用FSDP

- **模型非常大，单个GPU显存无法容纳完整模型时**，FSDP通过参数切分降低单卡显存需求，使得训练更大规模模型成为可能[2][8][9]。
- **希望训练更大批量大小以提升训练效率**，FSDP减少内存占用，释放更多显存空间给更大批次[2][4]。
- **需要在有限GPU资源下扩展训练规模**，通过分布式切分技术提升显存利用率[5][7]。
- **对训练性能和内存效率有较高要求**，FSDP通过通信和计算重叠优化提升训练速度[5]。

## FSDP用来解决的主要问题

- **显存瓶颈问题**：传统DDP复制整个模型参数和状态，导致显存占用大，限制了模型规模和批量大小。FSDP通过完全切分参数、梯度和优化器状态，显著降低单GPU显存需求[2][4]。
- **大模型训练难题**：随着模型参数数量激增，单机单卡训练难以满足需求，FSDP使得训练数十亿甚至上千亿参数模型成为可能[9]。
- **训练效率与内存利用的平衡**：FSDP通过动态收集和释放参数，结合通信与计算重叠，提升训练效率同时降低峰值显存使用[3][5]。

---

综上，FullyShardedDataParallel（FSDP）是一种先进的分布式训练技术，适用于训练超大规模深度学习模型，尤其在显存受限的多GPU环境下，通过将模型参数、梯度和优化器状态完全切分分布，有效解决了显存瓶颈问题，支持更大模型和更大批量的训练，同时保持较高的训练效率

